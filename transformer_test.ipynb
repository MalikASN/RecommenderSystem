{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf1ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc815c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üß† D√©marrage de l'Encodage LOURD ---\n",
      "üçú Pr√©paration de la Feature Soup...\n",
      "‚úÖ Soup cr√©√©e pour 5449 films.\n",
      "‚öôÔ∏è Utilisation du p√©riph√©rique: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G√©n√©ration des Embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [02:40<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matrice de vecteurs bruts g√©n√©r√©e: (5449, 384)\n",
      "--- üèÅ Initialisation Termin√©e ---\n",
      "\n",
      "--- Recommandations pour la s√©lection de 5 films (Top 5) ---\n",
      "Score      | Movie Title\n",
      "----------------------------------------\n",
      "0.6923     | xXx\n",
      "0.6906     | Tenet\n",
      "0.6884     | 6 Underground\n",
      "0.6831     | John Wick: Chapter 2\n",
      "0.6807     | 8MM\n",
      "\n",
      "--- Recommandations pour la s√©lection de 5 films (Top 5) ---\n",
      "Score      | Movie Title\n",
      "----------------------------------------\n",
      "0.6686     | Zookeeper\n",
      "0.6615     | Jumanji\n",
      "0.6599     | The Wild\n",
      "0.6544     | Wildlife\n",
      "0.6432     | Toy Story 2\n",
      "\n",
      "--- Recommandations pour la s√©lection de 5 films (Top 5) ---\n",
      "Score      | Movie Title\n",
      "----------------------------------------\n",
      "0.6426     | Click\n",
      "0.6373     | The International\n",
      "0.6329     | The Contract\n",
      "0.6272     | Maps to the Stars\n",
      "0.6230     | Spartacus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1. CONFIGURATION GLOBALE\n",
    "class Config:\n",
    "    \"\"\"Configuration du mod√®le et des chemins.\"\"\"\n",
    "    # Mod√®le BERT-based adapt√© au calcul de similarit√© s√©mantique\n",
    "    MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2' \n",
    "    CSV_PATH = 'IMDb_clean.csv'\n",
    "    BATCH_SIZE = 32  # Taille du lot pour l'encodage (optimisation m√©moire)\n",
    "    MAX_LENGTH = 128 # Longueur maximale de la s√©quence de tokens\n",
    "\n",
    "# Variables globales pour la persistance en m√©moire\n",
    "df_movies = None\n",
    "movie_title_to_index = None\n",
    "movie_vectors_array = None \n",
    "# La matrice N x N de similarit√© n'est plus n√©cessaire car on la calcule \n",
    "# √† la vol√©e entre le vecteur utilisateur (1 x D) et les vecteurs bruts (N x D).\n",
    "\n",
    "# 2. FONCTIONS D'UTILITAIRES ET DE PR√âPARATION\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"\n",
    "    Aggr√®ge les vecteurs de tokens en un seul vecteur de phrase\n",
    "    par la moyenne pond√©r√©e (manuel - l'√©tape \"Option B\").\n",
    "    \"\"\"\n",
    "    # R√©cup√®re les embeddings bruts de tous les tokens\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    \n",
    "    # √âtend le masque d'attention pour correspondre √† la taille des embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    \n",
    "    # Somme les embeddings (en multipliant par le masque, les z√©ros sont ignor√©s)\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    \n",
    "    # Somme le masque (compte le nombre de tokens valides)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    # Retourne la moyenne\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "def prepare_feature_soup(df):\n",
    "    \"\"\"\n",
    "    Nettoie les donn√©es et cr√©e la cha√Æne de caract√®res structur√©e ('soup') \n",
    "    pour le mod√®le Transformer.\n",
    "    \"\"\"\n",
    "    print(\" Pr√©paration de la Feature Soup...\")\n",
    "    \n",
    "    # 1. Gestion des valeurs manquantes et nettoyage de base\n",
    "    df['description'] = df['description'].fillna('')\n",
    "    df['side_genre'] = df['side_genre'].fillna('')\n",
    "    df['Director'] = df['Director'].fillna('Unknown')\n",
    "    \n",
    "    # Nettoyage des espaces exc√©dentaires\n",
    "    for col in ['main_genre', 'side_genre', 'Actors', 'Movie_Title', 'Director']:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "    \n",
    "    # 2. Cr√©ation de la \"Soup\" structur√©e pour le mod√®le\n",
    "    df['soup'] = (\n",
    "        \"Title: \" + df['Movie_Title'] + \". \" +\n",
    "        \"Director: \" + df['Director'] + \". \" +\n",
    "        \"Genres: \" + df['main_genre'] + \", \" + df['side_genre'] + \". \" +\n",
    "        \"Stars: \" + df['Actors'] + \". \" +\n",
    "        \"Plot: \" + df['description']\n",
    "    )\n",
    "    \n",
    "    # Nettoyage final de la string\n",
    "    df['soup'] = df['soup'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    \n",
    "    print(f\"‚úÖ Soup cr√©√©e pour {len(df)} films.\")\n",
    "    return df\n",
    "\n",
    "# 3. INITIALISATION (La partie LOURDE - Ex√©cut√©e une seule fois)\n",
    "\n",
    "def initialize_and_encode():\n",
    "    \"\"\"\n",
    "    Ex√©cute les √©tapes lourdes (encodage) et stocke les r√©sultats \n",
    "    dans les variables globales.\n",
    "    \"\"\"\n",
    "    global df_movies, movie_title_to_index, movie_vectors_array\n",
    "    \n",
    "    if movie_vectors_array is not None:\n",
    "        print(\"‚úÖ Vecteurs d√©j√† encod√©s en m√©moire. Saut de l'√©tape lourde.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n---  D√©marrage de l'Encodage LOURD ---\")\n",
    "    \n",
    "    # A. Chargement et Pr√©paration\n",
    "    df = pd.read_csv(Config.CSV_PATH)\n",
    "    df_movies = prepare_feature_soup(df.copy())\n",
    "    \n",
    "    # Cr√©e le mapping Titre -> Index (cl√© essentielle)\n",
    "    movie_title_to_index = pd.Series(df_movies.index, index=df_movies['Movie_Title']).drop_duplicates()\n",
    "    \n",
    "    # B. Configuration du Mod√®le et de l'Environnement\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\" Utilisation du p√©riph√©rique: {device}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "    model = AutoModel.from_pretrained(Config.MODEL_NAME).to(device).eval() # Mode √©valuation\n",
    "    \n",
    "    all_embeddings = []\n",
    "    texts = df_movies['soup'].tolist()\n",
    "\n",
    "    # C. Boucle d'Encodage\n",
    "    for i in tqdm(range(0, len(texts), Config.BATCH_SIZE), desc=\"G√©n√©ration des Embeddings\"):\n",
    "        batch_texts = texts[i : i + Config.BATCH_SIZE]\n",
    "        \n",
    "        # 1. Tokenization (Hugging Face)\n",
    "        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, \n",
    "                                  max_length=Config.MAX_LENGTH, return_tensors='pt').to(device)\n",
    "        \n",
    "        # 2. Mod√®le Inference (PyTorch / Transformer)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "            \n",
    "        # 3. Pooling (Agr√©gation manuelle) et Normalisation\n",
    "        batch_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
    "        \n",
    "        all_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "\n",
    "    movie_vectors_array = np.vstack(all_embeddings)\n",
    "    print(f\"‚úÖ Matrice de vecteurs bruts g√©n√©r√©e: {movie_vectors_array.shape}\")\n",
    "    print(\"--- üèÅ Initialisation Termin√©e ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee0e0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
